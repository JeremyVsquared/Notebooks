{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.initializers import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils.generic_utils import Progbar\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'data/wgan'\n",
    "BATCH_SIZE = 100\n",
    "ITERATIONS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# use all available 70k samples\n",
    "X_train = np.concatenate((X_train, X_test))\n",
    "y_train = np.concatenate((y_train, y_test))\n",
    "\n",
    "# convert to -1..1 range, reshape to (sample_i, 28, 28, 1)\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = np.expand_dims(X_train, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (70000, 28, 28, 1)\n",
      "Train samples: 70000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"Train samples: {}\".format(X_train.shape[0]))\n",
    "print(\"Test samples: {}\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(y, y_pred):\n",
    "    return K.mean(y * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    weight_init = RandomNormal(mean=0., stddev=0.02)\n",
    "    input_image = Input(shape=(28, 28, 1), name='input_image')\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), padding='same', name='conv_1', kernel_initializer=weight_init)(input_image)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding='same', name='conv_2', kernel_initializer=weight_init)(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), padding='same', name='conv_3', kernel_initializer=weight_init)(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), padding='same', name='conv_4', kernel_initializer=weight_init)(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    features = Flatten()(x)\n",
    "    \n",
    "    output_is_fake = Dense(1, activation='linear', name='output_is_fake')(features)\n",
    "    \n",
    "    output_class = Dense(10, activation='softmax', name='output_class')(features)\n",
    "    \n",
    "    return Model(inputs=[input_image], outputs=[output_is_fake, output_class], name='discriminator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z_size=100):\n",
    "    weight_init = RandomNormal(mean=0., stddev=0.02)\n",
    "    \n",
    "    input_class = Input(shape=(1,), dtype='int32', name='input_class')\n",
    "    \n",
    "    e = Embedding(10, z_size, embeddings_initializer='glorot_uniform')(input_class)\n",
    "    embedded_class = Flatten(name='embedded_class')(e)\n",
    "    \n",
    "    input_z = Input(shape=(z_size,), name='input_z')\n",
    "    \n",
    "    h = multiply([input_z, embedded_class], name='h')\n",
    "    \n",
    "    x = Dense(1024)(h)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Dense(128*7*7)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Reshape((7, 7, 128))(x)\n",
    "    \n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(256, (5, 5), padding='same', kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(128, (5, 5), padding='same', kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Conv2D(1, (5, 5), padding='same', name='output_generated_image', kernel_initializer=weight_init)(x)\n",
    "    \n",
    "    return Model(inputs=[input_z, input_class], outputs=x, name='generator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking discriminator and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = discriminator()\n",
    "d.compile(optimizer=RMSprop(lr=0.00005), loss=[discriminator_loss, 'sparse_categorical_crossentropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_z = Input(shape=(100,), name='input_z_')\n",
    "input_class = Input(shape=(1,), name='input_class_', dtype='int32')\n",
    "\n",
    "g = generator()\n",
    "\n",
    "output_is_fake, output_class = d(g(inputs=[input_z, input_class]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = Model(inputs=[input_z, input_class], outputs=[output_is_fake, output_class])\n",
    "\n",
    "dg.compile(optimizer=RMSprop(lr=0.00005), loss=[discriminator_loss, 'sparse_categorical_crossentropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_z_ (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_class_ (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "generator (Model)               (None, 28, 28, 1)    8175209     input_z_[0][0]                   \n",
      "                                                                 input_class_[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "discriminator (Model)           [(None, 1), (None, 1 390667      generator[1][0]                  \n",
      "==================================================================================================\n",
      "Total params: 8,565,876\n",
      "Trainable params: 8,565,876\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 10x10 sample of generated images\n",
    "samples_zz = np.random.normal(0., 1., (100, 100))\n",
    "def generate_samples(n=0, save=True):\n",
    "    generated_classes = np.array(list(range(0, 10)) * 10)\n",
    "    generated_images = G.predict([samples_zz, generated_classes.reshape(-1, 1)])\n",
    "\n",
    "    rr = []\n",
    "    for c in range(10):\n",
    "        rr.append(\n",
    "            np.concatenate(generated_images[c * 10:(1 + c) * 10]).reshape(\n",
    "                280, 28))\n",
    "    img = np.hstack(rr)\n",
    "\n",
    "    if save:\n",
    "        plt.imsave(OUT_DIR + '/samples_%07d.png' % n, img, cmap=plt.cm.gray)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write tensorboard summaries\n",
    "sw = tf.summary.FileWriter(DIR)\n",
    "def update_tb_summary(step, sample_images=True, save_image_files=True):\n",
    "\n",
    "    s = tf.Summary()\n",
    "\n",
    "    # losses as is\n",
    "    for names, vals in zip((('D_real_is_fake', 'D_real_class'),\n",
    "                            ('D_fake_is_fake', 'D_fake_class'), \n",
    "                            ('DG_is_fake', 'DG_class')),\n",
    "                           (D_true_losses, D_fake_losses, DG_losses)):\n",
    "\n",
    "        v = s.value.add()\n",
    "        v.simple_value = vals[-1][1]\n",
    "        v.tag = names[0]\n",
    "\n",
    "        v = s.value.add()\n",
    "        v.simple_value = vals[-1][2]\n",
    "        v.tag = names[1]\n",
    "\n",
    "    # D loss: -1*D_true_is_fake - D_fake_is_fake\n",
    "    v = s.value.add()\n",
    "    v.simple_value = -D_true_losses[-1][1] - D_fake_losses[-1][1]\n",
    "    v.tag = 'D loss (-1*D_real_is_fake - D_fake_is_fake)'\n",
    "\n",
    "    # generated image\n",
    "    if sample_images:\n",
    "        img = generate_samples(step, save=save_image_files)\n",
    "        s.MergeFromString(tf.Session().run(\n",
    "            tf.summary.image('samples_%07d' % step,\n",
    "                             img.reshape([1, img.shape, 1]))))\n",
    "\n",
    "    sw.add_summary(s, step)\n",
    "    sw.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake = 1\n",
    "# real = -1\n",
    "\n",
    "progress_bar = Progbar(target=ITERATIONS)\n",
    "\n",
    "DG_losses = []\n",
    "D_true_losses = []\n",
    "D_fake_losses = []\n",
    "\n",
    "for it in range(ITERATIONS):\n",
    "\n",
    "    if len(D_true_losses) > 0:\n",
    "        progress_bar.update(\n",
    "            it,\n",
    "            values=[\n",
    "                    ('D_real_is_fake', np.mean(D_true_losses[-5:], axis=0)[1]),\n",
    "                    ('D_real_class', np.mean(D_true_losses[-5:], axis=0)[2]),\n",
    "                    ('D_fake_is_fake', np.mean(D_fake_losses[-5:], axis=0)[1]),\n",
    "                    ('D_fake_class', np.mean(D_fake_losses[-5:], axis=0)[2]),\n",
    "                    ('D(G)_is_fake', np.mean(DG_losses[-5:],axis=0)[1]),\n",
    "                    ('D(G)_class', np.mean(DG_losses[-5:],axis=0)[2])\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        progress_bar.update(it)\n",
    "\n",
    "    # 1: train D on real+generated images\n",
    "\n",
    "    if (it % 1000) < 25 or it % 500 == 0: # 25 times in 1000, every 500th\n",
    "        d_iters = 100\n",
    "    else:\n",
    "        d_iters = D_ITERS\n",
    "\n",
    "    for d_it in range(d_iters):\n",
    "\n",
    "        # unfreeze D\n",
    "        D.trainable = True\n",
    "        for l in D.layers: l.trainable = True\n",
    "            \n",
    "        # # restore D dropout rates\n",
    "        # for l in D.layers:\n",
    "        #     if l.name.startswith('dropout'):\n",
    "        #         l.rate = l._rate\n",
    "\n",
    "        # clip D weights\n",
    "\n",
    "        for l in D.layers:\n",
    "            weights = l.get_weights()\n",
    "            weights = [np.clip(w, -0.01, 0.01) for w in weights]\n",
    "            l.set_weights(weights)\n",
    "\n",
    "        # 1.1: maximize D output on reals === minimize -1*(D(real))\n",
    "\n",
    "        # draw random samples from real images\n",
    "        index = np.random.choice(len(X_train), BATCH_SIZE, replace=False)\n",
    "        real_images = X_train[index]\n",
    "        real_images_classes = y_train[index]\n",
    "\n",
    "        D_loss = D.train_on_batch(real_images, [-np.ones(BATCH_SIZE), real_images_classes])\n",
    "        D_true_losses.append(D_loss)\n",
    "\n",
    "        # 1.2: minimize D output on fakes \n",
    "\n",
    "        zz = np.random.normal(0., 1., (BATCH_SIZE, Z_SIZE))\n",
    "        generated_classes = np.random.randint(0, 10, BATCH_SIZE)\n",
    "        generated_images = G.predict([zz, generated_classes.reshape(-1, 1)])\n",
    "\n",
    "        D_loss = D.train_on_batch(generated_images, [np.ones(BATCH_SIZE), generated_classes])\n",
    "        D_fake_losses.append(D_loss)\n",
    "\n",
    "    # 2: train D(G) (D is frozen)\n",
    "    # minimize D output while supplying it with fakes, telling it that they are reals (-1)\n",
    "\n",
    "    # freeze D\n",
    "    D.trainable = False\n",
    "    for l in D.layers: l.trainable = False\n",
    "        \n",
    "    # # disable D dropout layers\n",
    "    # for l in D.layers:\n",
    "    #     if l.name.startswith('dropout'):\n",
    "    #         l.rate = 0.\n",
    "\n",
    "    zz = np.random.normal(0., 1., (BATCH_SIZE, Z_SIZE)) \n",
    "    generated_classes = np.random.randint(0, 10, BATCH_SIZE)\n",
    "\n",
    "    DG_loss = DG.train_on_batch(\n",
    "        [zz, generated_classes.reshape((-1, 1))],\n",
    "        [-np.ones(BATCH_SIZE), generated_classes])\n",
    "\n",
    "    DG_losses.append(DG_loss)\n",
    "\n",
    "    if it % 10 == 0:\n",
    "        update_tb_summary(it, sample_images=(it % 10 == 0), save_image_files=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
